diff --git a/include/llama.h b/include/llama.h
index a4abf395..e03a11de 100644
--- a/include/llama.h
+++ b/include/llama.h
@@ -439,6 +439,7 @@ extern "C" {
     LLAMA_API uint32_t llama_n_batch    (const struct llama_context * ctx);
     LLAMA_API uint32_t llama_n_ubatch   (const struct llama_context * ctx);
     LLAMA_API uint32_t llama_n_seq_max  (const struct llama_context * ctx);
+    LLAMA_API bool llama_supports_embedding_only (const struct llama_context * ctx);
 
     LLAMA_API int32_t llama_n_vocab    (const struct llama_model * model);
     LLAMA_API int32_t llama_n_ctx_train(const struct llama_model * model);
diff --git a/src/llama.cpp b/src/llama.cpp
index b442781a..7096a486 100644
--- a/src/llama.cpp
+++ b/src/llama.cpp
@@ -20431,9 +20431,17 @@ struct llama_context * llama_new_context_with_model(
 
     // this is necessary due to kv_self.n being padded later during inference
     cparams.n_ctx            = GGML_PAD(cparams.n_ctx, llama_kv_cache_get_padding(cparams));
+    if (!hparams.causal_attn) {
+        LLAMA_LOG_WARN("%s: adjust n_ctx of the none causal attention model to the minimum value between given n_ctx and n_ctx_train", __func__);
+        cparams.n_ctx        = std::min(cparams.n_ctx, hparams.n_ctx_train);
+    }
 
     // with causal attention, the batch size is limited by the context size
     cparams.n_batch          = hparams.causal_attn ? std::min(cparams.n_ctx, params.n_batch) : params.n_batch;
+    if (!hparams.causal_attn) {
+        LLAMA_LOG_WARN("%s: align n_batch of the none causal attention model to n_ctx", __func__);
+        cparams.n_batch      = cparams.n_ctx;
+    }
 
     // the batch has to be at least GGML_KQ_MASK_PAD because we will be padding the KQ_mask
     // this is required by GPU kernels in order to avoid out-of-bounds accesses (e.g. ggml_flash_attn_ext)
@@ -20444,6 +20452,10 @@ struct llama_context * llama_new_context_with_model(
     }
 
     cparams.n_ubatch         = std::min(cparams.n_batch, params.n_ubatch == 0 ? params.n_batch : params.n_ubatch);
+    if (!hparams.causal_attn) {
+        LLAMA_LOG_WARN("%s: align n_ubatch of the none causal attention model to n_ctx", __func__);
+        cparams.n_ubatch     = cparams.n_ctx;
+    }
 
     cparams.n_ctx_orig_yarn  = params.yarn_orig_ctx    != 0 ? params.yarn_orig_ctx    :
                                hparams.n_ctx_orig_yarn != 0 ? hparams.n_ctx_orig_yarn :
@@ -20743,6 +20755,10 @@ uint32_t llama_n_seq_max(const struct llama_context * ctx) {
     return ctx->kv_self.size;
 }
 
+bool llama_supports_embedding_only(const struct llama_context * ctx) {
+    return !ctx->cparams.causal_attn;
+}
+
 enum llama_vocab_type llama_vocab_type(const struct llama_model * model) {
     return model->vocab.type;
 }
