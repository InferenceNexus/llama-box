diff --git a/common/common.cpp b/common/common.cpp
index 20be9291..cfbfb179 100644
--- a/common/common.cpp
+++ b/common/common.cpp
@@ -1614,27 +1614,34 @@ std::string common_detokenize(llama_context * ctx, const std::vector<llama_token
 
 bool common_chat_verify_template(const std::string & tmpl) {
     llama_chat_message chat[] = {{"user", "test"}};
-    int res = llama_chat_apply_template(nullptr, tmpl.c_str(), chat, 1, true, nullptr, 0);
+    int res = llama_chat_apply_template(nullptr, tmpl.c_str(), chat, 1, nullptr, 0, false, true, nullptr, 0);
     return res >= 0;
 }
 
 std::string common_chat_apply_template(const struct llama_model * model,
         const std::string & tmpl,
         const std::vector<common_chat_msg> & msgs,
+        const std::vector<common_chat_func> & funcs,
+        bool req_func,
         bool add_ass) {
     int alloc_size = 0;
     bool fallback = false; // indicate if we must fallback to default chatml
     std::vector<llama_chat_message> chat;
-    for (auto & msg : msgs) {
+    for (const auto & msg : msgs) {
         chat.push_back({msg.role.c_str(), msg.content.c_str()});
         alloc_size += (msg.role.size() + msg.content.size()) * 1.25;
     }
+    std::vector<llama_chat_function> func;
+    for (const auto & fn: funcs) {
+        func.push_back({fn.name.c_str(), fn.description.c_str(), fn.parameter.c_str()});
+        alloc_size += (fn.name.size() + fn.description.size() + fn.parameter.size()) * 1.25;
+    }
 
     const char * ptr_tmpl = tmpl.empty() ? nullptr : tmpl.c_str();
     std::vector<char> buf(alloc_size);
 
     // run the first time to get the total output length
-    int32_t res = llama_chat_apply_template(model, ptr_tmpl, chat.data(), chat.size(), add_ass, buf.data(), buf.size());
+    int32_t res = llama_chat_apply_template(model, ptr_tmpl, chat.data(), chat.size(), func.data(), func.size(), req_func, add_ass, buf.data(), buf.size());
 
     // error: chat template is not supported
     if (res < 0) {
@@ -1644,7 +1651,7 @@ std::string common_chat_apply_template(const struct llama_model * model,
             throw std::runtime_error("this custom template is not supported");
         } else {
             // If the built-in template is not supported, we default to chatml
-            res = llama_chat_apply_template(nullptr, "chatml", chat.data(), chat.size(), add_ass, buf.data(), buf.size());
+            res = llama_chat_apply_template(nullptr, "chatml", chat.data(), chat.size(), func.data(), func.size(), req_func, add_ass, buf.data(), buf.size());
             fallback = true;
         }
     }
@@ -1655,7 +1662,7 @@ std::string common_chat_apply_template(const struct llama_model * model,
         res = llama_chat_apply_template(
             fallback ? nullptr : model,
             fallback ? "chatml" : ptr_tmpl,
-            chat.data(), chat.size(), add_ass, buf.data(), buf.size());
+            chat.data(), chat.size(), func.data(), func.size(), req_func, add_ass, buf.data(), buf.size());
     }
 
     std::string formatted_chat(buf.data(), res);
@@ -1668,7 +1675,7 @@ std::string common_chat_format_single(const struct llama_model * model,
         const common_chat_msg & new_msg,
         bool add_ass) {
     std::ostringstream ss;
-    auto fmt_past_msg = past_msg.empty() ? "" : common_chat_apply_template(model, tmpl, past_msg, false);
+    auto fmt_past_msg = past_msg.empty() ? "" : common_chat_apply_template(model, tmpl, past_msg, {}, false, false);
     std::vector<common_chat_msg> chat_new(past_msg);
     // if the past_msg ends with a newline, we must preserve it in the formatted version
     if (add_ass && !fmt_past_msg.empty() && fmt_past_msg.back() == '\n') {
@@ -1676,7 +1683,7 @@ std::string common_chat_format_single(const struct llama_model * model,
     };
     // format chat with new_msg
     chat_new.push_back(new_msg);
-    auto fmt_new_msg = common_chat_apply_template(model, tmpl, chat_new, add_ass);
+    auto fmt_new_msg = common_chat_apply_template(model, tmpl, chat_new, {}, false, add_ass);
     // get the diff part
     ss << fmt_new_msg.substr(fmt_past_msg.size(), fmt_new_msg.size() - fmt_past_msg.size());
     return ss.str();
@@ -1685,12 +1692,16 @@ std::string common_chat_format_single(const struct llama_model * model,
 std::string common_chat_format_example(const struct llama_model * model,
         const std::string & tmpl) {
     std::vector<common_chat_msg> msgs = {
-        {"system",    "You are a helpful assistant"},
-        {"user",      "Hello"},
-        {"assistant", "Hi there"},
-        {"user",      "How are you?"},
+        {"system",    "You are a helpful assistant."},
+        {"user",      "Hello."},
+        {"assistant", "Hi there."},
+        {"user",      "What's the weather like in Paris today?"},
+    };
+    std::vector<common_chat_func> funcs = {
+        {"get_weather", "", "{\"type\":\"object\",\"properties\":{\"location\":{\"type\":\"string\"}}}"},
+        {"get_temperature", "Return the temperature according to the location.", "{\"type\":\"object\",\"properties\":{\"location\":{\"type\":\"string\"}}}"},
     };
-    return common_chat_apply_template(model, tmpl, msgs, true);
+    return common_chat_apply_template(model, tmpl, msgs, funcs, false, true);
 }
 
 //
diff --git a/common/common.h b/common/common.h
index 1d2bd932..3f350c46 100644
--- a/common/common.h
+++ b/common/common.h
@@ -571,6 +571,13 @@ struct common_chat_msg {
     std::string content;
 };
 
+// same with llama_chat_function, but uses std::string
+struct common_chat_func {
+    std::string name;
+    std::string description;
+    std::string parameter;
+};
+
 // Check if the template supplied via "--chat-template" is supported or not. Returns true if it's valid
 bool common_chat_verify_template(const std::string & tmpl);
 
@@ -580,6 +587,8 @@ bool common_chat_verify_template(const std::string & tmpl);
 std::string common_chat_apply_template(const struct llama_model * model,
         const std::string & tmpl,
         const std::vector<common_chat_msg> & chat,
+        const std::vector<common_chat_func> & tools,
+        bool req_func,
         bool add_ass);
 
 // Format single message, while taking into account the position of that message in chat history
diff --git a/include/llama.h b/include/llama.h
index a4abf395..1af8c141 100644
--- a/include/llama.h
+++ b/include/llama.h
@@ -383,6 +383,11 @@ extern "C" {
         const char * role;
         const char * content;
     } llama_chat_message;
+    typedef struct llama_chat_function {
+        const char * name;
+        const char * description;
+        const char * parameters;
+    } llama_chat_function;
 
     // lora adapter
     struct llama_lora_adapter;
@@ -979,6 +984,8 @@ extern "C" {
     /// @param tmpl A Jinja template to use for this chat. If this is nullptr, the modelâ€™s default chat template will be used instead.
     /// @param chat Pointer to a list of multiple llama_chat_message
     /// @param n_msg Number of llama_chat_message in this chat
+    /// @param func Pointer to a list of multiple llama_chat_function (In JSON format)
+    /// @param n_func Number of llama_chat_function in this chat
     /// @param add_ass Whether to end the prompt with the token(s) that indicate the start of an assistant message.
     /// @param buf A buffer to hold the output formatted prompt. The recommended alloc size is 2 * (total number of characters of all messages)
     /// @param length The size of the allocated buffer
@@ -988,6 +995,9 @@ extern "C" {
                             const char * tmpl,
        const struct llama_chat_message * chat,
                                 size_t   n_msg,
+      const struct llama_chat_function * func,
+                                size_t   n_func,
+                                  bool   req_func,
                                   bool   add_ass,
                                   char * buf,
                                int32_t   length);
@@ -995,6 +1005,9 @@ extern "C" {
     // Get list of built-in chat templates
     LLAMA_API int32_t llama_chat_builtin_templates(const char ** output, size_t len);
 
+    // Get chat template alias
+    LLAMA_API const char * llama_chat_template_alias(const char * tmpl);
+
     //
     // Sampling API
     //
diff --git a/src/llama.cpp b/src/llama.cpp
index 4d41602f..b764f158 100644
--- a/src/llama.cpp
+++ b/src/llama.cpp
@@ -22945,14 +22945,114 @@ static llm_chat_template llama_chat_detect_template(const std::string & tmpl) {
 // Simple version of "llama_apply_chat_template" that only works with strings
 // This function uses heuristic checks to determine commonly used template. It is not a jinja parser.
 static int32_t llama_chat_apply_template_internal(
+    const llm_arch arch,
     const llm_chat_template tmpl,
     const std::vector<const llama_chat_message *> & chat,
-    std::string & dest, bool add_ass) {
+    const std::vector<const llama_chat_function * > & func,
+    std::string & dest, bool req_func, bool add_ass) {
     // Taken from the research: https://github.com/ggerganov/llama.cpp/issues/5527
     std::stringstream ss;
     if (tmpl == LLM_CHAT_TEMPLATE_CHATML) {
+        if (!func.empty()) {
+            const llama_chat_message *root_msg = nullptr;
+            for (const auto *message: chat) {
+                std::string role(message->role);
+                if (role == "system") {
+                    root_msg = message;
+                    break;
+                }
+            }
+            ss << "<|im_start|>system\n";
+            if (root_msg) {
+                ss << root_msg->content << "\n\n";
+            } else {
+                ss << "You are a helpful assistant.\n\n";
+            }
+            if (arch == LLM_ARCH_QWEN2VL) {
+                ss << "## Tools\n\n";
+            } else {
+                ss << "# Tools\n\n";
+            }
+            if (req_func) {
+                ss << "You MUST call one or more functions to assist with the user query. Do not make assumptions about what values to plug into functions.\n\n";
+            } else {
+                ss << "You CAN call functions to assist with the user query. Do not make assumptions about what values to plug into functions.\n\n";
+            }
+            if (arch == LLM_ARCH_QWEN2VL) {
+                ss << "You are provided with following function tools:\n\n";
+                for (const auto & fn : func) {
+                    ss << "### " << fn->name << "\n\n";
+                    ss << fn->name << ": " << fn->description << " Parameters: " << fn->parameters << "Format the arguments as a JSON object.\n\n";
+                }
+                if (req_func) {
+                    ss << "For each function call, return a JSON object with function name and arguments within <tool_call></tool_call> XML tags:\n";
+                } else {
+                    ss << "When you can reply with your internal knowledge, reply directly without any function calls. ";
+                    ss << "Otherwise, for each function call, return a JSON object with function name and arguments within <tool_call></tool_call> XML tags:\n";
+                }
+                ss << "<tool_call>\n";
+                ss << "{\"name\": The name of the function to use, \"arguments\": The input of the function, must be a JSON object in compact format}\n";
+                ss << "</tool_call>\n";
+                ss << "<tool_result>\n";
+                ss << "The function results.\n";
+                ss << "</tool_result>\n";
+                ss << "Reply based on the function results." << "<|im_end|>\n";
+            } else {
+                ss << "You are provided with following function signatures within <tools></tools> XML tags:\n";
+                ss << "<tools>\n";
+                for (const auto & fn : func) {
+                    ss << R"({"type": "function", "function": {"name": ")" << fn->name << R"(", "description": ")" << fn->description << R"(", "parameters": )" << fn->parameters << "}}\n";
+                }
+                ss << "</tools>\n\n";
+                if (req_func) {
+                    ss << "For each function call, return a JSON object with function name and arguments within <tool_call></tool_call> XML tags:\n";
+                } else {
+                    ss << "When you can reply with your internal knowledge, reply directly without any function calls. ";
+                    ss << "Otherwise, for each function call, return a JSON object with function name and arguments within <tool_call></tool_call> XML tags:\n";
+                }
+                ss << "<tool_call>\n";
+                ss << "{\"name\": <function-name>, \"arguments\": <arguments-json-object>}\n";
+                ss << "</tool_call>" << "<|im_end|>\n";
+            }
+        }
+        bool previous_tool_response = false;
         // chatml template
         for (auto message : chat) {
+            if (!func.empty()) {
+                std::string role(message->role);
+                if (role == "assistant_tool_call") {
+                    if (arch == LLM_ARCH_QWEN2VL) {
+                        if (!previous_tool_response) {
+                            ss << "<|im_start|>assistant\n";
+                        }
+                        ss << "<tool_call>\n" << message->content << "\n</tool_call>\n";
+                    } else {
+                        ss << "<|im_start|>assistant\n";
+                        ss << "<tool_call>\n" << message->content << "\n</tool_call>";
+                        ss << "<|im_end|>\n";
+                    }
+                    previous_tool_response = false;
+                    continue;
+                }
+                previous_tool_response = false;
+                if (role == "system") {
+                    continue;
+                }
+                if (role == "tool") {
+                    if (arch == LLM_ARCH_QWEN2VL) {
+                        ss << "<tool_result>\n" << message->content << "\n</tool_result>\n";
+                        add_ass = message != chat.back();
+                    } else {
+                        ss << "<|im_start|>user\n" << message->content << "<|im_end|>\n";
+                    }
+                    previous_tool_response = true;
+                    continue;
+                }
+                if (role == "assistant" && arch == LLM_ARCH_QWEN2VL) {
+                    ss << message->content << "<|im_end|>\n";
+                    continue;
+                }
+            }
             ss << "<|im_start|>" << message->role << "\n" << message->content << "<|im_end|>\n";
         }
         if (add_ass) {
@@ -22961,15 +23061,59 @@ static int32_t llama_chat_apply_template_internal(
     } else if (tmpl == LLM_CHAT_TEMPLATE_MISTRAL_V7) {
         // Official mistral 'v7' template
         // See: https://huggingface.co/mistralai/Mistral-Large-Instruct-2411#basic-instruct-template-v7
+        // See: https://github.com/mistralai/mistral-common/releases/tag/v1.5.0
+        if (!func.empty()) {
+            const llama_chat_message *root_msg = nullptr;
+            for (const auto *message: chat) {
+                std::string role(message->role);
+                if (role == "system") {
+                    root_msg = message;
+                    break;
+                }
+            }
+            ss << "[AVAILABLE_TOOLS] " << "[";
+            for (const auto & fn : func) {
+                ss << R"({"type": "function", "function": {"name": ")" << fn->name << R"(", "description": ")" << fn->description << R"(", "parameters": )" << fn->parameters << "}}";
+                ss << ((fn == func.back()) ? "" : ",");
+            }
+            ss << "]" << "[/AVAILABLE_TOOLS]";
+            if (root_msg) {
+                ss << "[SYSTEM_PROMPT] " << root_msg->content;
+            } else {
+                ss << "[SYSTEM_PROMPT] " << "You are a helpful assistant. ";
+            }
+            if (req_func) {
+                ss << "You MUST call one or more functions to assist with the user query. Do not make assumptions about what values to plug into functions. ";
+            } else {
+                ss << "You CAN call functions to assist with the user query. Do not make assumptions about what values to plug into functions. ";
+                ss << "When you can reply with your internal knowledge, reply directly without any function calls. ";
+                ss << "Otherwise, try to call functions to assist with the user query. ";
+            }
+            ss << "[/SYSTEM_PROMPT]";
+        }
         for (auto message : chat) {
             std::string role(message->role);
             std::string content(message->content);
             if (role == "system") {
+                if (!func.empty()) {
+                    continue;
+                }
                 ss << "[SYSTEM_PROMPT] " << content << "[/SYSTEM_PROMPT]";
             } else if (role == "user") {
                 ss << "[INST] " << content << "[/INST]";
             }
             else {
+                if (!func.empty()) {
+                    if (role == "assistant_tool_call") {
+                        ss << "[TOOL_CALLS] ";
+                        ss << "[" << message->content << "]</s>";
+                        continue;
+                    }
+                    if (role == "tool") {
+                        ss << "[TOOL_RESULTS] " << message->content << "[/TOOL_RESULTS]";
+                        continue;
+                    }
+                }
                 ss << " " << content << "</s>";
             }
         }
@@ -22978,10 +23122,41 @@ static int32_t llama_chat_apply_template_internal(
             || tmpl == LLM_CHAT_TEMPLATE_MISTRAL_V3_TEKKEN) {
         // See: https://github.com/mistralai/cookbook/blob/main/concept-deep-dive/tokenization/chat_templates.md
         // See: https://github.com/mistralai/cookbook/blob/main/concept-deep-dive/tokenization/templates.md
+        // See: https://github.com/mistralai/cookbook/blob/main/concept-deep-dive/tokenization/tool_calling.md
         std::string leading_space = tmpl == LLM_CHAT_TEMPLATE_MISTRAL_V1 ? " " : "";
         std::string trailing_space = tmpl == LLM_CHAT_TEMPLATE_MISTRAL_V3_TEKKEN ? "" : " ";
         bool trim_assistant_message = tmpl == LLM_CHAT_TEMPLATE_MISTRAL_V3;
         bool is_inside_turn = false;
+        if (!func.empty()) {
+            const llama_chat_message *root_msg = nullptr;
+            for (const auto *message: chat) {
+                std::string role(message->role);
+                if (role == "system") {
+                    root_msg = message;
+                    break;
+                }
+            }
+            ss << leading_space << "[AVAILABLE_TOOLS]" << trailing_space << "[";
+            for (const auto & fn : func) {
+                ss << R"({"type": "function", "function": {"name": ")" << fn->name << R"(", "description": ")" << fn->description << R"(", "parameters": )" << fn->parameters << "}}";
+                ss << ((fn == func.back()) ? "" : ",");
+            }
+            ss << "]" << leading_space << "[/AVAILABLE_TOOLS]";
+            ss << leading_space << "[INST]" << trailing_space;
+            is_inside_turn = true;
+            if (root_msg) {
+                ss << root_msg->content << "\n\n";
+            } else {
+                ss << "You are a helpful assistant.\n\n";
+            }
+            if (req_func) {
+                ss << "You MUST call one or more functions to assist with the user query. Do not make assumptions about what values to plug into functions.\n\n";
+            } else {
+                ss << "You CAN call functions to assist with the user query. Do not make assumptions about what values to plug into functions. ";
+                ss << "When you can reply with your internal knowledge, reply directly without any function calls. ";
+                ss << "Otherwise, try to call functions to assist with the user query.\n\n";
+            }
+        }
         for (auto message : chat) {
             if (!is_inside_turn) {
                 ss << leading_space << "[INST]" << trailing_space;
@@ -22990,10 +23165,26 @@ static int32_t llama_chat_apply_template_internal(
             std::string role(message->role);
             std::string content(message->content);
             if (role == "system") {
+                if (!func.empty()) {
+                    continue;
+                }
                 ss << content << "\n\n";
             } else if (role == "user") {
                 ss << content << leading_space << "[/INST]";
             } else {
+                if (!func.empty()) {
+                    if (role == "assistant_tool_call") {
+                        ss << leading_space << "[TOOL_CALLS]" << trailing_space;
+                        ss << "[" << message->content << "]</s>";
+                        continue;
+                    }
+                    if (role == "tool") {
+                        ss << leading_space << "[TOOL_RESULTS]" << trailing_space;
+                        ss << message->content;
+                        ss << leading_space << "[/TOOL_RESULTS]";
+                        continue;
+                    }
+                }
                 ss << trailing_space << (trim_assistant_message ? trim(content) : content) << "</s>";
                 is_inside_turn = false;
             }
@@ -23177,9 +23368,61 @@ static int32_t llama_chat_apply_template_internal(
             ss << "<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>";
         }
     } else if (tmpl == LLM_CHAT_TEMPLATE_LLAMA_3) {
+        if (!func.empty()) {
+            const llama_chat_message *root_msg = nullptr;
+            for (const auto *message: chat) {
+                std::string role(message->role);
+                if (role == "system") {
+                    root_msg = message;
+                    break;
+                }
+            }
+            ss << "<|start_header_id|>system<|end_header_id|>\n\n";
+            if (root_msg) {
+                ss << trim(root_msg->content) << "\n\n";
+            } else {
+                ss << "You are a helpful assistant.\n\n";
+            }
+            if (req_func) {
+                ss << "You MUST call one or more functions to assist with the user query. Do not make assumptions about what values to plug into functions.";
+            } else {
+                ss << "You CAN call functions to assist with the user query. Do not make assumptions about what values to plug into functions.";
+            }
+            ss << "<|eot_id|>";
+        }
         // Llama 3
         for (auto message : chat) {
             std::string role(message->role);
+            if (!func.empty()) {
+                if (role == "system") {
+                    continue;
+                }
+                if (role == "assistant_tool_call") {
+                    ss << "<|start_header_id|>assistant<|end_header_id|>\n\n" << trim(message->content) << "<|eot_id|>";
+                    continue;
+                }
+                if (role == "tool") {
+                    ss << "<|start_header_id|>ipython<|end_header_id|>\n\n" << trim(message->content) << "<|eot_id|>";
+                    continue;
+                }
+                if (role == "user" && message == chat.back()) {
+                    ss << "<|start_header_id|>user<|end_header_id|>\n\n";
+                    ss << "You are provided with following function signatures within <tools></tools> XML tags:\n";
+                    ss << "<tools>\n";
+                    for (const auto & fn : func) {
+                        ss << R"({"type": "function", "function": {"name": ")" << fn->name << R"(", "description": ")" << fn->description << R"(", "parameters": )" << fn->parameters << "}}\n";
+                    }
+                    ss << "</tools>\n\n";
+                    if (req_func) {
+                        ss << "For each function call, return a JSON object with function name and arguments in the format {\"name\": <function-name>, \"arguments\": <arguments-json-object>}.\n";
+                    } else {
+                        ss << "When you can reply with your internal knowledge, reply directly without any function call. ";
+                        ss << "Otherwise, for each function call, return a JSON object with function name and arguments in the format {\"name\": <function-name>, \"arguments\": <arguments-json-object>}.\n";
+                    }
+                    ss << trim(message->content) << "<|eot_id|>";
+                    continue;
+                }
+            }
             ss << "<|start_header_id|>" << role << "<|end_header_id|>\n\n" << trim(message->content) << "<|eot_id|>";
         }
         if (add_ass) {
@@ -23258,13 +23501,59 @@ static int32_t llama_chat_apply_template_internal(
             }
         }
     } else if (tmpl == LLM_CHAT_TEMPLATE_GRANITE) {
+        if (!func.empty()) {
+            const llama_chat_message *root_msg = nullptr;
+            for (const auto *message: chat) {
+                std::string role(message->role);
+                if (role == "system") {
+                    root_msg = message;
+                    break;
+                }
+            }
+            ss << "<|start_of_role|>tools<|end_of_role|>[";
+            for (const auto & fn : func) {
+                ss << R"({"type": "function", "function": {"name": ")" << fn->name << R"(", "description": ")" << fn->description << R"(", "parameters": )" << fn->parameters << "}}";
+                ss << ((fn == func.back()) ? "" : ",");
+            }
+            ss << "]<|end_of_text|>\n";
+            ss << "<|start_of_role|>system<|end_of_role|>";
+            if (root_msg) {
+                ss << trim(root_msg->content) << " ";
+            } else {
+                ss << "You are a helpful assistant with tool calling capabilities. ";
+            }
+            if (req_func) {
+                ss << "You MUST call one or more tools to assist with the user query. Do not make assumptions about what values to plug into tools. ";
+            } else {
+                ss << "You CAN call tools to assist with the user query. Do not make assumptions about what values to plug into tools. ";
+            }
+            if (req_func) {
+                ss << "For each tool call, return <|tool_call|><tool_call> followed by a JSON list of tool used as follows: ";
+            } else {
+                ss << "When you can reply with your internal knowledge, reply directly without any tool calls. ";
+                ss << "Otherwise, for each tool call, return <|tool_call|><tool_call> followed by a JSON list of tool used as follows: ";
+            }
+            ss << R"(<|tool_call|><tool_call>[{"name": <function-name>, "arguments": <arguments-json-object>}])";
+            ss << "Write the response to the user's input by strictly aligning with the facts in the provided documents.";
+            ss << "<|end_of_text|>\n";
+        }
         // IBM Granite template
         for (const auto & message : chat) {
             std::string role(message->role);
-            ss << "<|start_of_role|>" << role << "<|end_of_role|>";
-            if (role == "assistant_tool_call") {
-                ss << "<|tool_call|>";
+            if (!func.empty()) {
+                if (role == "system") {
+                    continue;
+                }
+                if (role == "assistant_tool_call") {
+                    ss << "<|start_of_role|>assistant<|start_of_role|><|tool_call|><tool_call>" << message->content << "<|end_of_text|>\n";
+                    continue;
+                }
+                if (role == "tool") {
+                    ss << "<|start_of_role|>tool_response<|end_of_role|>" << message->content << "<|end_of_text|>\n";
+                    continue;
+                }
             }
+            ss << "<|start_of_role|>" << role << "<|end_of_role|>";
             ss << message->content << "<|end_of_text|>\n";
         }
         if (add_ass) {
@@ -23319,6 +23608,9 @@ int32_t llama_chat_apply_template(
                               const char * tmpl,
          const struct llama_chat_message * chat,
                                   size_t   n_msg,
+        const struct llama_chat_function * func,
+                                  size_t   n_func,
+                                    bool   req_func,
                                     bool   add_ass,
                                     char * buf,
                                  int32_t   length) {
@@ -23344,12 +23636,19 @@ int32_t llama_chat_apply_template(
         chat_vec[i] = &chat[i];
     }
 
+    // format the func to string
+    std::vector<const llama_chat_function *> func_vec;
+    func_vec.resize(n_func);
+    for (size_t i = 0; i < n_func; i++) {
+        func_vec[i] = &func[i];
+    }
+
     std::string formatted_chat;
     llm_chat_template detected_tmpl = llama_chat_detect_template(curr_tmpl);
     if (detected_tmpl == LLM_CHAT_TEMPLATE_UNKNOWN) {
         return -1;
     }
-    int32_t res = llama_chat_apply_template_internal(detected_tmpl, chat_vec, formatted_chat, add_ass);
+    int32_t res = llama_chat_apply_template_internal(model->arch, detected_tmpl, chat_vec, func_vec, formatted_chat, req_func, add_ass);
     if (res < 0) {
         return res;
     }
@@ -23368,6 +23667,17 @@ int32_t llama_chat_builtin_templates(const char ** output, size_t len) {
     return (int32_t) LLM_CHAT_TEMPLATES.size();
 }
 
+const char * llama_chat_template_alias(const char * tmpl) {
+    llm_chat_template t = llama_chat_detect_template(std::string(tmpl));
+    for (const auto & it : LLM_CHAT_TEMPLATES) {
+        if (it.second != t) {
+            continue;
+        }
+        return it.first.c_str();
+    }
+    return "unknown";
+}
+
 //
 // sampling
 //
