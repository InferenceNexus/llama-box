diff --git a/common/common.cpp b/common/common.cpp
index c0c98232..106b6f78 100644
--- a/common/common.cpp
+++ b/common/common.cpp
@@ -1613,27 +1613,33 @@ std::string common_detokenize(llama_context * ctx, const std::vector<llama_token
 
 bool common_chat_verify_template(const std::string & tmpl) {
     llama_chat_message chat[] = {{"user", "test"}};
-    int res = llama_chat_apply_template(nullptr, tmpl.c_str(), chat, 1, true, nullptr, 0);
+    int res = llama_chat_apply_template(nullptr, tmpl.c_str(), chat, 1, nullptr, 0, true, nullptr, 0);
     return res >= 0;
 }
 
 std::string common_chat_apply_template(const struct llama_model * model,
         const std::string & tmpl,
         const std::vector<common_chat_msg> & msgs,
+        const std::vector<common_chat_func> & funcs,
         bool add_ass) {
     int alloc_size = 0;
     bool fallback = false; // indicate if we must fallback to default chatml
     std::vector<llama_chat_message> chat;
-    for (auto & msg : msgs) {
+    for (const auto & msg : msgs) {
         chat.push_back({msg.role.c_str(), msg.content.c_str()});
         alloc_size += (msg.role.size() + msg.content.size()) * 1.25;
     }
+    std::vector<llama_chat_function> func;
+    for (const auto & fn: funcs) {
+        func.push_back({fn.name.c_str(), fn.description.c_str(), fn.parameter.c_str()});
+        alloc_size += (fn.name.size() + fn.description.size() + fn.parameter.size()) * 1.25;
+    }
 
     const char * ptr_tmpl = tmpl.empty() ? nullptr : tmpl.c_str();
     std::vector<char> buf(alloc_size);
 
     // run the first time to get the total output length
-    int32_t res = llama_chat_apply_template(model, ptr_tmpl, chat.data(), chat.size(), add_ass, buf.data(), buf.size());
+    int32_t res = llama_chat_apply_template(model, ptr_tmpl, chat.data(), chat.size(), func.data(), func.size(), add_ass, buf.data(), buf.size());
 
     // error: chat template is not supported
     if (res < 0) {
@@ -1643,7 +1649,7 @@ std::string common_chat_apply_template(const struct llama_model * model,
             throw std::runtime_error("this custom template is not supported");
         } else {
             // If the built-in template is not supported, we default to chatml
-            res = llama_chat_apply_template(nullptr, "chatml", chat.data(), chat.size(), add_ass, buf.data(), buf.size());
+            res = llama_chat_apply_template(nullptr, "chatml", chat.data(), chat.size(), func.data(), func.size(), add_ass, buf.data(), buf.size());
             fallback = true;
         }
     }
@@ -1654,7 +1660,7 @@ std::string common_chat_apply_template(const struct llama_model * model,
         res = llama_chat_apply_template(
             fallback ? nullptr : model,
             fallback ? "chatml" : ptr_tmpl,
-            chat.data(), chat.size(), add_ass, buf.data(), buf.size());
+            chat.data(), chat.size(), func.data(), func.size(), add_ass, buf.data(), buf.size());
     }
 
     std::string formatted_chat(buf.data(), res);
@@ -1667,7 +1673,7 @@ std::string common_chat_format_single(const struct llama_model * model,
         const common_chat_msg & new_msg,
         bool add_ass) {
     std::ostringstream ss;
-    auto fmt_past_msg = past_msg.empty() ? "" : common_chat_apply_template(model, tmpl, past_msg, false);
+    auto fmt_past_msg = past_msg.empty() ? "" : common_chat_apply_template(model, tmpl, past_msg, {}, false);
     std::vector<common_chat_msg> chat_new(past_msg);
     // if the past_msg ends with a newline, we must preserve it in the formatted version
     if (add_ass && !fmt_past_msg.empty() && fmt_past_msg.back() == '\n') {
@@ -1675,7 +1681,7 @@ std::string common_chat_format_single(const struct llama_model * model,
     };
     // format chat with new_msg
     chat_new.push_back(new_msg);
-    auto fmt_new_msg = common_chat_apply_template(model, tmpl, chat_new, add_ass);
+    auto fmt_new_msg = common_chat_apply_template(model, tmpl, chat_new, {}, add_ass);
     // get the diff part
     ss << fmt_new_msg.substr(fmt_past_msg.size(), fmt_new_msg.size() - fmt_past_msg.size());
     return ss.str();
@@ -1684,12 +1690,16 @@ std::string common_chat_format_single(const struct llama_model * model,
 std::string common_chat_format_example(const struct llama_model * model,
         const std::string & tmpl) {
     std::vector<common_chat_msg> msgs = {
-        {"system",    "You are a helpful assistant"},
-        {"user",      "Hello"},
-        {"assistant", "Hi there"},
-        {"user",      "How are you?"},
+        {"system",    "You are a helpful assistant."},
+        {"user",      "Hello."},
+        {"assistant", "Hi there."},
+        {"user",      "What's the weather like in Paris today?"},
+    };
+    std::vector<common_chat_func> funcs = {
+        {"get_weather", "", "{\"type\":\"object\",\"properties\":{\"location\":{\"type\":\"string\"}}}"},
+        {"get_temperature", "Return the temperature according to the location.", "{\"type\":\"object\",\"properties\":{\"location\":{\"type\":\"string\"}}}"},
     };
-    return common_chat_apply_template(model, tmpl, msgs, true);
+    return common_chat_apply_template(model, tmpl, msgs, funcs, true);
 }
 
 //
diff --git a/common/common.h b/common/common.h
index 5f556c24..45cb8bae 100644
--- a/common/common.h
+++ b/common/common.h
@@ -560,6 +560,13 @@ struct common_chat_msg {
     std::string content;
 };
 
+// same with llama_chat_function, but uses std::string
+struct common_chat_func {
+    std::string name;
+    std::string description;
+    std::string parameter;
+};
+
 // Check if the template supplied via "--chat-template" is supported or not. Returns true if it's valid
 bool common_chat_verify_template(const std::string & tmpl);
 
@@ -569,6 +576,7 @@ bool common_chat_verify_template(const std::string & tmpl);
 std::string common_chat_apply_template(const struct llama_model * model,
         const std::string & tmpl,
         const std::vector<common_chat_msg> & chat,
+        const std::vector<common_chat_func> & tools,
         bool add_ass);
 
 // Format single message, while taking into account the position of that message in chat history
diff --git a/include/llama.h b/include/llama.h
index efbb27d2..19031e22 100644
--- a/include/llama.h
+++ b/include/llama.h
@@ -383,6 +383,11 @@ extern "C" {
         const char * role;
         const char * content;
     } llama_chat_message;
+    typedef struct llama_chat_function {
+        const char * name;
+        const char * description;
+        const char * parameters;
+    } llama_chat_function;
 
     // lora adapter
     struct llama_lora_adapter;
@@ -982,6 +987,8 @@ extern "C" {
     /// @param tmpl A Jinja template to use for this chat. If this is nullptr, the modelâ€™s default chat template will be used instead.
     /// @param chat Pointer to a list of multiple llama_chat_message
     /// @param n_msg Number of llama_chat_message in this chat
+    /// @param func Pointer to a list of multiple llama_chat_function (In JSON format)
+    /// @param n_func Number of llama_chat_function in this chat
     /// @param add_ass Whether to end the prompt with the token(s) that indicate the start of an assistant message.
     /// @param buf A buffer to hold the output formatted prompt. The recommended alloc size is 2 * (total number of characters of all messages)
     /// @param length The size of the allocated buffer
@@ -991,6 +998,8 @@ extern "C" {
                             const char * tmpl,
        const struct llama_chat_message * chat,
                                 size_t   n_msg,
+      const struct llama_chat_function * func,
+                                size_t   n_func,
                                   bool   add_ass,
                                   char * buf,
                                int32_t   length);
diff --git a/src/llama.cpp b/src/llama.cpp
index b7b04a41..a742db1b 100644
--- a/src/llama.cpp
+++ b/src/llama.cpp
@@ -22274,12 +22274,52 @@ static llm_chat_template llama_chat_detect_template(const std::string & tmpl) {
 static int32_t llama_chat_apply_template_internal(
     const llm_chat_template tmpl,
     const std::vector<const llama_chat_message *> & chat,
+    const std::vector<const llama_chat_function * > & func,
     std::string & dest, bool add_ass) {
     // Taken from the research: https://github.com/ggerganov/llama.cpp/issues/5527
     std::stringstream ss;
     if (tmpl == LLM_CHAT_TEMPLATE_CHATML) {
         // chatml template
+        if (!func.empty()) {
+            const llama_chat_message *root_msg = nullptr;
+            for (const auto *message: chat) {
+                std::string role(message->role);
+                if (role == "system") {
+                    root_msg = message;
+                    break;
+                }
+            }
+            ss << "<|im_start|>system\n";
+            if (root_msg) {
+                ss << root_msg->content << "\n";
+            } else {
+                ss << "You are a helpful assistant.\n";
+            }
+            ss << "# Tools\n";
+            ss << "You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions.\n";
+            ss << "You are provided with following function signatures within <tools></tools> XML tags:\n";
+            ss << "<tools>\n";
+            for (const auto & fn : func) {
+                ss << "{\"type\": \"function\", \"function\": {\"name\": \"" << fn->name << "\", \"description\": \"" << fn->description << "\", \"parameters\": " << fn->parameters << "}}\n";
+            }
+            ss << "</tools>\n";
+            ss << "Use the following pydantic model json schema for each tool call you will make: {\"properties\": {\"arguments\": {\"title\": \"Arguments\", \"type\": \"object\"}, \"name\": {\"title\": \"Name\", \"type\": \"string\"}}, \"required\": [\"arguments\", \"name\"], \"title\": \"FunctionCall\", \"type\": \"object\"} For each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags as follows:\n";
+            ss << "<tool_call>\n";
+            ss << "{\"name\": <function-name>, \"arguments\": <args-json-object>}\n";
+            ss << "</tool_call>" << "<|im_end|>\n";
+        }
+
         for (auto message : chat) {
+            if (!func.empty()) {
+                std::string role(message->role);
+                if (role == "system") {
+                    continue;
+                }
+                if (role == "tool") {
+                    ss << "<|im_start|>user\n" << message->content << "<|im_end|>\n";
+                    continue;
+                }
+            }
             ss << "<|im_start|>" << message->role << "\n" << message->content << "<|im_end|>\n";
         }
         if (add_ass) {
@@ -22627,6 +22667,8 @@ int32_t llama_chat_apply_template(
                               const char * tmpl,
          const struct llama_chat_message * chat,
                                   size_t   n_msg,
+        const struct llama_chat_function * func,
+                                  size_t   n_func,
                                     bool   add_ass,
                                     char * buf,
                                  int32_t   length) {
@@ -22652,12 +22694,19 @@ int32_t llama_chat_apply_template(
         chat_vec[i] = &chat[i];
     }
 
+    // format the func to string
+    std::vector<const llama_chat_function *> func_vec;
+    func_vec.resize(n_func);
+    for (size_t i = 0; i < n_func; i++) {
+        func_vec[i] = &func[i];
+    }
+
     std::string formatted_chat;
     llm_chat_template detected_tmpl = llama_chat_detect_template(curr_tmpl);
     if (detected_tmpl == LLM_CHAT_TEMPLATE_UNKNOWN) {
         return -1;
     }
-    int32_t res = llama_chat_apply_template_internal(detected_tmpl, chat_vec, formatted_chat, add_ass);
+    int32_t res = llama_chat_apply_template_internal(detected_tmpl, chat_vec, func_vec, formatted_chat, add_ass);
     if (res < 0) {
         return res;
     }
