diff --git a/common/common.cpp b/common/common.cpp
index 4bb140ee..a73a7939 100644
--- a/common/common.cpp
+++ b/common/common.cpp
@@ -881,6 +881,11 @@ struct common_init_result common_init_from_params(common_params & params) {
 
     auto cparams = common_context_params_to_llama(params);
 
+    if (llama_model_needs_mrope(model)) {
+        LOG_INF("%s: model requires M-RoPE, increasing batch size by 4x\n", __func__);
+        cparams.n_batch *= 4;
+    }
+
     llama_context * lctx = llama_new_context_with_model(model, cparams);
     if (lctx == NULL) {
         LOG_ERR("%s: failed to create context with model '%s'\n", __func__, params.model.c_str());
diff --git a/include/llama.h b/include/llama.h
index 7b305b29..596dcd19 100644
--- a/include/llama.h
+++ b/include/llama.h
@@ -494,6 +494,9 @@ extern "C" {
     // to the decoder to start generating output sequence. For other models, it returns -1.
     LLAMA_API llama_token llama_model_decoder_start_token(const struct llama_model * model);
 
+    // Returns true if the model needs M-RoPE (like Qwen2VL, etc.)
+    LLAMA_API bool llama_model_needs_mrope(const struct llama_model * model);
+
     // Returns true if the model is recurrent (like Mamba, RWKV, etc.)
     LLAMA_API bool llama_model_is_recurrent(const struct llama_model * model);
 
diff --git a/src/llama-model.cpp b/src/llama-model.cpp
index ace0ba26..adf14311 100644
--- a/src/llama-model.cpp
+++ b/src/llama-model.cpp
@@ -2155,6 +2155,10 @@ llama_token llama_model_decoder_start_token(const struct llama_model * model) {
     return model->hparams.dec_start_token_id;
 }
 
+bool llama_model_needs_mrope(const struct llama_model * model) {
+    return std::any_of(model->hparams.rope_sections.begin(), model->hparams.rope_sections.end(), [](const int32_t & x) { return x != 0; });
+}
+
 bool llama_model_is_recurrent(const struct llama_model * model) {
     switch (model->arch) {
         case LLM_ARCH_MAMBA:  return true;
