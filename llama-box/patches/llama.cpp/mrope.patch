diff --git a/common/common.cpp b/common/common.cpp
index 20be9291..8711792b 100644
--- a/common/common.cpp
+++ b/common/common.cpp
@@ -878,6 +878,11 @@ struct common_init_result common_init_from_params(common_params & params) {
 
     auto cparams = common_context_params_to_llama(params);
 
+    if (llama_model_needs_mrope(model)) {
+        LOG_INF("%s: model requires M-RoPE, increasing batch size by 4x\n", __func__);
+        cparams.n_batch *= 4;
+    }
+
     llama_context * lctx = llama_new_context_with_model(model, cparams);
     if (lctx == NULL) {
         LOG_ERR("%s: failed to create context with model '%s'\n", __func__, params.model.c_str());
diff --git a/include/llama.h b/include/llama.h
index a4abf395..b129103c 100644
--- a/include/llama.h
+++ b/include/llama.h
@@ -495,6 +495,9 @@ extern "C" {
     // Returns true if the model is recurrent (like Mamba, RWKV, etc.)
     LLAMA_API bool llama_model_is_recurrent(const struct llama_model * model);
 
+    // Returns true if the model needs M-RoPE (like Qwen2VL, etc.)
+    LLAMA_API bool llama_model_needs_mrope(const struct llama_model * model);
+
     // Returns 0 on success
     LLAMA_API uint32_t llama_model_quantize(
             const char * fname_inp,
diff --git a/src/llama.cpp b/src/llama.cpp
index 4d41602f..1d22b12a 100644
--- a/src/llama.cpp
+++ b/src/llama.cpp
@@ -21207,6 +21207,10 @@ bool llama_model_is_recurrent(const struct llama_model * model) {
     }
 }
 
+bool llama_model_needs_mrope(const struct llama_model * model) {
+    return std::any_of(model->hparams.rope_sections, model->hparams.rope_sections + 4, [](const int32_t & x) { return x != 0; });
+}
+
 uint32_t llama_model_quantize(
         const char * fname_inp,
         const char * fname_out,
