diff --git a/src/llama.cpp b/src/llama.cpp
index 4d41602f..cf2a7922 100644
--- a/src/llama.cpp
+++ b/src/llama.cpp
@@ -1720,6 +1720,9 @@ enum llm_chat_template {
     LLM_CHAT_TEMPLATE_RWKV_WORLD,
     LLM_CHAT_TEMPLATE_GRANITE,
     LLM_CHAT_TEMPLATE_GIGACHAT,
+    LLM_CHAT_TEMPLATE_FALCON,
+    LLM_CHAT_TEMPLATE_LLAVA,
+    LLM_CHAT_TEMPLATE_LLAVA_MISTRAL,
     LLM_CHAT_TEMPLATE_MEGREZ,
     LLM_CHAT_TEMPLATE_UNKNOWN,
 };
@@ -1755,6 +1758,9 @@ static const std::map<std::string, llm_chat_template> LLM_CHAT_TEMPLATES = {
     { "granite",           LLM_CHAT_TEMPLATE_GRANITE           },
     { "gigachat",          LLM_CHAT_TEMPLATE_GIGACHAT          },
     { "megrez",            LLM_CHAT_TEMPLATE_MEGREZ            },
+    { "falcon",            LLM_CHAT_TEMPLATE_FALCON            },
+    { "llava",             LLM_CHAT_TEMPLATE_LLAVA             },
+    { "llava-mistral",     LLM_CHAT_TEMPLATE_LLAVA_MISTRAL     },
 };
 
 static llm_arch llm_arch_from_string(const std::string & name) {
@@ -22938,6 +22944,8 @@ static llm_chat_template llama_chat_detect_template(const std::string & tmpl) {
         return LLM_CHAT_TEMPLATE_GIGACHAT;
     } else if (tmpl_contains("<|role_start|>")) {
         return LLM_CHAT_TEMPLATE_MEGREZ;
+    }  else if ((tmpl_contains("Falcon:") && tmpl_contains("message['content']"))) {
+        return LLM_CHAT_TEMPLATE_FALCON;
     }
     return LLM_CHAT_TEMPLATE_UNKNOWN;
 }
@@ -23306,6 +23314,61 @@ static int32_t llama_chat_apply_template_internal(
         if (add_ass) {
             ss << "<|role_start|>assistant<|role_end|>";
         }
+    } else if (tmpl == LLM_CHAT_TEMPLATE_FALCON) {
+        // Falcon
+        std::string system_prompt;
+        for (const auto &message : chat) {
+            std::string role(message->role);
+            if (role == "system") {
+                system_prompt = trim(message->content);
+            } else if (role == "user") {
+                if (!system_prompt.empty()) {
+                    ss << "\n" << system_prompt << "\n\n\n";
+                    system_prompt = "";
+                }
+                ss << "\nUser: " << trim(message->content) << "\n\n\n";
+            } else if (role == "assistant") {
+                ss << "\nAssistant: " << trim(message->content) << "\n\n\n";
+            }
+        }
+        if (add_ass) {
+            ss << "\nAssistant:";
+        }
+    } else if (tmpl == LLM_CHAT_TEMPLATE_LLAVA || tmpl == LLM_CHAT_TEMPLATE_LLAVA_MISTRAL) {
+        // llava 1.5
+        if (tmpl != LLM_CHAT_TEMPLATE_LLAVA_MISTRAL) {
+            ss << "A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions.\n";
+        }
+        for (auto message : chat) {
+            std::string role(message->role);
+            if (role == "user") {
+                std::string content(message->content);
+                if (tmpl != LLM_CHAT_TEMPLATE_LLAVA_MISTRAL) {
+                    ss << "USER:" << message->content << "\n";
+                } else {
+                    const std::string sign = "<image>\n";
+                    const size_t sign_pos = content.find(sign);
+                    if (sign_pos != std::string::npos) {
+                        content = content.replace(sign_pos, sign.size(), "");
+                        ss << "<image>\n";
+                    }
+                    ss << "USER:\n" << content.c_str() << "\n";
+                }
+            } else if (role == "assistant") {
+                if (tmpl != LLM_CHAT_TEMPLATE_LLAVA_MISTRAL) {
+                    ss << "ASSISTANT:" << message->content << "</s>\n";
+                } else {
+                    ss << "ASSISTANT:\n" << message->content << "</s>\n";
+                }
+            }
+        }
+        if (add_ass) {
+            if (tmpl != LLM_CHAT_TEMPLATE_LLAVA_MISTRAL) {
+                ss << "ASSISTANT:";
+            } else {
+                ss << "ASSISTANT:\n";
+            }
+        }
     } else {
         // template not supported
         return -1;
